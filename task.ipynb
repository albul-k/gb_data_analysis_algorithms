{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ca35beca1e73f0e8e48de5d26c91c8a581bc78491f6978c6ebd776970508bb03"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### *Измените функцию calc_logloss так, чтобы нули по возможности не попадали в np.log (как вариант - np.clip).\n",
    "### Подберите аргументы функции eval_LR_model для логистической регрессии таким образом, чтобы log loss был минимальным.\n",
    "### Создайте функцию calc_pred_proba, возвращающую предсказанную вероятность класса 1 (на вход подаются веса, которые уже посчитаны функцией eval_LR_model и X, на выходе - массив y_pred_proba).\n",
    "### Создайте функцию calc_pred, возвращающую предсказанный класс (на вход подаются веса, которые уже посчитаны функцией eval_LR_model и X, на выходе - массив y_pred).\n",
    "### Посчитайте accuracy, матрицу ошибок, precision и recall, а также F1-score.\n",
    "### Могла ли модель переобучиться? Почему?\n",
    "### *Создайте функции eval_LR_model_l1 и eval_LR_model_l2 с применением L1 и L2 регуляризации соответственно."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "              [1, 1, 2, 1, 3, 0, 5, 10, 1, 2],  # стаж\n",
    "              [500, 700, 750, 600, 1450,        # средняя стоимость занятия\n",
    "               800, 1500, 2000, 450, 1000],\n",
    "              [1, 1, 2, 1, 2, 1, 3, 3, 1, 2]], dtype = np.float64) # квалификация репетитора\n",
    "\n",
    "y = np.array([0, 0, 1, 0, 1, 0, 1, 0, 1, 1]) # подходит или нет репетитор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_std_feat(x):\n",
    "    res = (x - x.mean()) / x.std()\n",
    "    return res\n",
    "\n",
    "def calc_mse(y, y_pred):\n",
    "    err = np.mean((y - y_pred)**2)\n",
    "    return err\n",
    "\n",
    "def calc_logloss(y, y_pred):\n",
    "    eps = 1e-15\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    err = np.mean(- y * np.log(y_pred) - (1.0 - y) * np.log(1.0 - y_pred))\n",
    "    return err\n",
    "\n",
    "def sigmoid(z):\n",
    "    res = 1 / (1 + np.exp(-z))\n",
    "    return res\n",
    "\n",
    "def eval_LR_model(X, y, iterations, alpha=1e-4, lambda_=1e-3, penalty=None):\n",
    "    np.random.seed(42)\n",
    "    w = np.random.randn(X.shape[0])\n",
    "    n = X.shape[1]\n",
    "    min_err_logloss = float('inf')\n",
    "    for i in range(1, iterations + 1):\n",
    "        \n",
    "        z = np.dot(w, X)\n",
    "        y_pred = sigmoid(z)\n",
    "        err_logloss = calc_logloss(y, y_pred)\n",
    "        if min(err_logloss, min_err_logloss) < min_err_logloss:\n",
    "            min_err_logloss = err_logloss\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        y_pred = np.dot(w, X)\n",
    "        err_mse = calc_mse(y, y_pred)\n",
    "    \n",
    "        if penalty == 'l2':\n",
    "            w -= alpha * (1/n * np.dot((y_pred - y), X.T) + 2 * lambda_ * w)\n",
    "        elif penalty == 'l1':\n",
    "            w -= alpha * (1/n * np.dot((y_pred - y), X.T) + lambda_ * abs(w))\n",
    "        else:\n",
    "            w -= alpha * (1/n * np.dot((y_pred - y), X.T))\n",
    "    return w, min_err_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_iter\talpha\tpenalty\terr_logloss\n",
      "\n",
      "500\t0.1\tl1\t0.7812040218425931\n",
      "500\t0.1\tl2\t0.7812499548693961\n",
      "500\t0.1\tNone\t0.7811211780116794\n",
      "500\t0.01\tl1\t0.5924901479443765\n",
      "500\t0.01\tl2\t0.5925957172495678\n",
      "500\t0.01\tNone\t0.5924438857850406\n",
      "500\t0.001\tl1\t0.60115125781248\n",
      "500\t0.001\tl2\t0.6011783423449634\n",
      "500\t0.001\tNone\t0.6011526187544622\n",
      "500\t0.0001\tl1\t0.7830961063600291\n",
      "500\t0.0001\tl2\t0.7831007049991416\n",
      "500\t0.0001\tNone\t0.7831301378886715\n",
      "500\t1e-05\tl1\t1.1512387324105906\n",
      "500\t1e-05\tl2\t1.1512363784057977\n",
      "500\t1e-05\tNone\t1.1512446696677174\n",
      "800\t0.1\tl1\t0.7812040218425931\n",
      "800\t0.1\tl2\t0.7812499548693961\n",
      "800\t0.1\tNone\t0.7811211780116794\n",
      "800\t0.01\tl1\t0.5924901479443765\n",
      "800\t0.01\tl2\t0.5925957172495678\n",
      "800\t0.01\tNone\t0.5924438857850406\n",
      "800\t0.001\tl1\t0.5948631899624397\n",
      "800\t0.001\tl2\t0.5949206496380038\n",
      "800\t0.001\tNone\t0.5948453040243635\n",
      "800\t0.0001\tl1\t0.6931530304542741\n",
      "800\t0.0001\tl2\t0.6931633850641272\n",
      "800\t0.0001\tNone\t0.6931865473756946\n",
      "800\t1e-05\tl1\t1.1132117788888256\n",
      "800\t1e-05\tl2\t1.1132085415086708\n",
      "800\t1e-05\tNone\t1.113221009182572\n",
      "1000\t0.1\tl1\t0.7812040218425931\n",
      "1000\t0.1\tl2\t0.7812499548693961\n",
      "1000\t0.1\tNone\t0.7811211780116794\n",
      "1000\t0.01\tl1\t0.5924901479443765\n",
      "1000\t0.01\tl2\t0.5925957172495678\n",
      "1000\t0.01\tNone\t0.5924438857850406\n",
      "1000\t0.001\tl1\t0.5931618723297586\n",
      "1000\t0.001\tl2\t0.5932408933027126\n",
      "1000\t0.001\tNone\t0.593131173956033\n",
      "1000\t0.0001\tl1\t0.6639738186304763\n",
      "1000\t0.0001\tl2\t0.6639849915614102\n",
      "1000\t0.0001\tNone\t0.6640043180338439\n",
      "1000\t1e-05\tl1\t1.089180735951696\n",
      "1000\t1e-05\tl2\t1.0891771047238443\n",
      "1000\t1e-05\tNone\t1.0891920468526797\n",
      "1200\t0.1\tl1\t0.7812040218425931\n",
      "1200\t0.1\tl2\t0.7812499548693961\n",
      "1200\t0.1\tNone\t0.7811211780116794\n",
      "1200\t0.01\tl1\t0.5924901479443765\n",
      "1200\t0.01\tl2\t0.5925957172495678\n",
      "1200\t0.01\tNone\t0.5924438857850406\n",
      "1200\t0.001\tl1\t0.5925753576032513\n",
      "1200\t0.001\tl2\t0.5926755281012894\n",
      "1200\t0.001\tNone\t0.592532306268691\n",
      "1200\t0.0001\tl1\t0.6467657543179413\n",
      "1200\t0.0001\tl2\t0.6467766612433915\n",
      "1200\t0.0001\tNone\t0.6467931287621036\n",
      "1200\t1e-05\tl1\t1.066164924745117\n",
      "1200\t1e-05\tl2\t1.0661610414977671\n",
      "1200\t1e-05\tNone\t1.066178224578054\n",
      "1400\t0.1\tl1\t0.7812040218425931\n",
      "1400\t0.1\tl2\t0.7812499548693961\n",
      "1400\t0.1\tNone\t0.7811211780116794\n",
      "1400\t0.01\tl1\t0.5924901479443765\n",
      "1400\t0.01\tl2\t0.5925957172495678\n",
      "1400\t0.01\tNone\t0.5924438857850406\n",
      "1400\t0.001\tl1\t0.5925490452231488\n",
      "1400\t0.001\tl2\t0.5926548581303688\n",
      "1400\t0.001\tNone\t0.5925023889219063\n",
      "1400\t0.0001\tl1\t0.6360656837785937\n",
      "1400\t0.0001\tl2\t0.6360760297761928\n",
      "1400\t0.0001\tNone\t0.6360903278440416\n",
      "1400\t1e-05\tl1\t1.0441308612203168\n",
      "1400\t1e-05\tl2\t1.0441268560300292\n",
      "1400\t1e-05\tNone\t1.0441460583136046\n",
      "\n",
      "Best params: {'err_logloss': 0.5924438857850406, 'weights': array([ 0.1544927 , -0.47062209,  0.62442963,  0.96986182]), 'params': {'n_iter': 500, 'alpha': 0.01, 'penalty': None}}\n"
     ]
    }
   ],
   "source": [
    "X_st = X.copy()\n",
    "X_st[2, :] = calc_std_feat(X[2, :])\n",
    "\n",
    "best_params = dict()\n",
    "min_err_logloss = float('inf')\n",
    "\n",
    "header = ('\\t').join(['n_iter','alpha','penalty','err_logloss'])\n",
    "print(f'{header}\\n')\n",
    "\n",
    "for n_iter in [500, 800, 1000, 1200, 1400]:\n",
    "    for alpha_ in [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]:\n",
    "        for penalty in ['l1', 'l2', None]:\n",
    "            w, err_logloss = eval_LR_model(X_st, y, iterations=n_iter, alpha=alpha_, lambda_=1e-3, penalty=penalty)\n",
    "            if min(err_logloss, min_err_logloss) < min_err_logloss:\n",
    "                min_err_logloss = err_logloss\n",
    "                best_params = {\n",
    "                    'err_logloss': err_logloss,\n",
    "                    'weights': w,\n",
    "                    'params': {\n",
    "                        'n_iter': n_iter,\n",
    "                        'alpha': alpha_,\n",
    "                        'penalty': penalty,\n",
    "                    }\n",
    "                }\n",
    "            print(f'{n_iter}\\t{alpha_}\\t{penalty}\\t{err_logloss}')\n",
    "print(f'\\nBest params: {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pred_proba(w, X):\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    return sigmoid(np.dot(w.T, X))\n",
    "\n",
    "def calc_pred(w, X):\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    y_pred = np.zeros((1, X.shape[1]))\n",
    "    y_ = sigmoid(np.dot(w.T, X))\n",
    "\n",
    "    for i in range(y_.shape[1]):\n",
    "        if (y_[:,i] > 0.5): \n",
    "            y_pred[:, i] = 1\n",
    "        elif (y_[:,i] <= 0.5):\n",
    "            y_pred[:, i] = 0\n",
    "    \n",
    "    return y_pred[0]\n",
    "\n",
    "def calc_metrics(y, y_pred):\n",
    "    l = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "    for idx, i in enumerate(y):\n",
    "        if y[idx] == y_pred[idx]:\n",
    "            l += 1\n",
    "        if y[idx] == 1 and y_pred[idx] == 1:\n",
    "            tp += 1\n",
    "        if y[idx] == 0 and y_pred[idx] == 0:\n",
    "            tn += 1\n",
    "        if y[idx] == 1 and y_pred[idx] == 0:\n",
    "            fn += 1\n",
    "        if y[idx] == 0 and y_pred[idx] == 1:\n",
    "            fp += 1\n",
    "    \n",
    "    accuracy = (l / len(y))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    matrix = np.array([[tp, fp], [fn, tn]])\n",
    "\n",
    "    return accuracy, precision, recall, f1, matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted classes: \n[1. 1. 1. 1. 1. 1. 1. 0. 0. 1.]\n\nPrediction probability: \n[[0.51051035 0.57434451 0.70333437 0.54260229 0.78482936 0.71074333\n  0.80011934 0.42014182 0.49441516 0.76587541]]\n\nAccuracy: 0.5\nPrecision: 0.5\nRecall: 0.8\nF1-score: 0.6153846153846154\nMatrix: \n[[4 4]\n [1 1]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = calc_pred(best_params[\"weights\"], X_st)\n",
    "y_pred_proba = calc_pred_proba(best_params[\"weights\"], X_st)\n",
    "\n",
    "accuracy, precision, recall, f1, matrix = calc_metrics(y.tolist(), y_pred.tolist())\n",
    "\n",
    "print(f'Predicted classes: \\n{y_pred}\\n')\n",
    "print(f'Prediction probability: \\n{y_pred_proba}\\n')\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-score: {f1}')\n",
    "print(f'Matrix: \\n{matrix}')"
   ]
  },
  {
   "source": [
    "### Могла ли модель переобучиться? Почему?\n",
    "Могла, так как имеем высокую долю false positive результатов."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}